# RNN-Music-Generation
RNN for music generation. Keras Functional API via tensorflow was used to create a neural network using gated recurrent units (GRU's)
# Data
The base dataset can be found [here](https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/); however, preprocessing was done on this dataset using music21 to remove both badly formatted data as well as data that contained notes not playable on the piano and notes too small for the given subdivision of 24 timesteps per quarter note. All in all, final data size was about 15500 songs, each stored in an individual pickle file. Each note had a corresponding number from 0-87, and the music was then converted to a seqence of timesteps. Timesteps in which more than one note was played were unrolled, starting with higher notes. 88 was used to represent the end of a given timestep. This formatting was necesarry to make the music compact and format it for input into an embedding layer. Before training, all the data was loaded into memory. The dataset is too large to be uploaded to github, but could be shared upon request
# Generation
Iterative generation was used. However, since the model was often unstable with regards to format, I cheated a bit and added a 60% to choose via argmax rather than distribution. While it may not be an accurate representation of the model's effectiveness, I found that it allowed the model to produce much better music.
# Results
Both midi files in this repo are outputs from the model. The model was trained overnight and ended up with a loss of around .4. The model has clearly learned which notes go together as well as some sense of rythym, however it can clearly work on its long term pattern recognition. Also, the model failed to produce samples greater than about 50 seconds long, as it began to deviate from format. Possible future steps involve using more GRU units or a GAN.
# Google Collab
The .ipynb file is set up to run in google collab, although file structures would have to be changed. Google collab was useful for me because of its access to free TPU resources that vastly outpaced my 2011 macbook pro. However, it had somewhat slow processing of non tensor related processing. To modify the code to not use collab, one would just have to change all the file references in the data gathering funtctions.
